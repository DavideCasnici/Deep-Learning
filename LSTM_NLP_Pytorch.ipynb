{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import zipfile\n",
        "import io"
      ],
      "metadata": {
        "id": "bEUTKHuHvspl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, pad_token=\"<pad>\", unk_token='<unk>'):\n",
        "        self.id_to_string = {}\n",
        "        self.string_to_id = {}\n",
        "\n",
        "        # add the default pad token\n",
        "        self.id_to_string[0] = pad_token\n",
        "        self.string_to_id[pad_token] = 0\n",
        "\n",
        "        # add the default unknown token\n",
        "        self.id_to_string[1] = unk_token\n",
        "        self.string_to_id[unk_token] = 1\n",
        "\n",
        "        # shortcut access\n",
        "        self.pad_id = 0\n",
        "        self.unk_id = 1\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.id_to_string)\n",
        "\n",
        "    def add_new_word(self, string):\n",
        "        self.string_to_id[string] = len(self.string_to_id)\n",
        "        self.id_to_string[len(self.id_to_string)] = string\n",
        "\n",
        "    # Given a string, return ID\n",
        "    def get_idx(self, string, extend_vocab=False):\n",
        "        if string in self.string_to_id:\n",
        "            return self.string_to_id[string]\n",
        "        elif extend_vocab:  # add the new word\n",
        "            self.add_new_word(string)\n",
        "            return self.string_to_id[string]\n",
        "        else:\n",
        "            return self.unk_id"
      ],
      "metadata": {
        "id": "ySWkJMRZvuV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the raw txt file and generate a 1D PyTorch tensor\n",
        "# containing the whole text mapped to sequence of token IDs, and a vocab object.\n",
        "class LongTextData:\n",
        "\n",
        "    def __init__(self, file_path, vocab=None, extend_vocab=True, device='cuda'):\n",
        "        self.data, self.vocab = self.text_to_data(file_path, vocab, extend_vocab, device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def text_to_data(self, text_file, vocab, extend_vocab, device):\n",
        "        assert os.path.exists(text_file)\n",
        "        if vocab is None:\n",
        "            vocab = Vocabulary()\n",
        "\n",
        "        data_list = []\n",
        "\n",
        "        # Construct data\n",
        "        full_text = []\n",
        "        print(f\"Reading text file from: {text_file}\")\n",
        "        with open(text_file, 'r') as text:\n",
        "            l_num = 0\n",
        "            cap_l = 0\n",
        "            contr = 0\n",
        "            for line in text:\n",
        "                l_num += 1\n",
        "                tokens = list(line)\n",
        "                for token in tokens:\n",
        "                    if token.isupper():\n",
        "                        cap_l += 1\n",
        "                    if token == \"â€™\" or token == \"'\":\n",
        "                        contr += 1\n",
        "                    full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "            print(\"Numbers of lines: \", l_num)\n",
        "        print(\"Vocabulary size: \", vocab.__len__())\n",
        "        print(\"Capital charachters: \", cap_l)\n",
        "        print(\"Number of contractions:\", contr)\n",
        "\n",
        "        data = torch.tensor(full_text, device=device, dtype=torch.int64)\n",
        "        print(\"Number of characters : \", data.__len__())\n",
        "        print(\"Avarage number of characters for line (int): \", math.trunc(data.__len__() / l_num))\n",
        "        print(\"Done.\")\n",
        "\n",
        "        return data, vocab\n",
        "\n",
        "    def string_to_data(self, text, device, pad_id):\n",
        "        extend_vocab = False\n",
        "        vocab = self.vocab\n",
        "\n",
        "        full_text = []\n",
        "        tokens = list(text)\n",
        "        for token in tokens:\n",
        "            full_text.append(vocab.get_idx(token, extend_vocab=extend_vocab))\n",
        "\n",
        "        data = torch.tensor(full_text, device=device, dtype=torch.int64)\n",
        "\n",
        "        text_len = len(text)\n",
        "        padded = data.data.new_full((text_len,), pad_id)\n",
        "        padded[:text_len] = data.data\n",
        "        padded = padded.view(1, text_len).t()\n",
        "\n",
        "        return padded"
      ],
      "metadata": {
        "id": "zRzvtYdhvxID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChunkedTextData:\n",
        "\n",
        "    def __init__(self, data, bsz, bptt_len, pad_id):\n",
        "        self.batches = self.create_batch(data, bsz, bptt_len, pad_id)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.batches[idx]\n",
        "\n",
        "    def create_batch(self, input_data, bsz, bptt_len, pad_id):\n",
        "        batches = []  # each element in `batches` is (len, B) tensor\n",
        "        text_len = len(input_data)\n",
        "        segment_len = text_len // bsz + 1\n",
        "\n",
        "        padded = input_data.data.new_full((segment_len * bsz,), pad_id)\n",
        "        padded[:text_len] = input_data.data\n",
        "        padded = padded.view(bsz, segment_len).t()\n",
        "        num_batches = segment_len // bptt_len + 1\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            if i == 0:\n",
        "                batch = torch.cat(\n",
        "                    [padded.new_full((1, bsz), pad_id),\n",
        "                     padded[i * bptt_len:(i + 1) * bptt_len]], dim=0)\n",
        "                batches.append(batch)\n",
        "            else:\n",
        "                batches.append(padded[i * bptt_len - 1:(i + 1) * bptt_len])\n",
        "\n",
        "        return batches"
      ],
      "metadata": {
        "id": "77GMz06UvyyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, voc_size):\n",
        "        super().__init__()\n",
        "        self.embedded_train = nn.Embedding(voc_size, 64, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(64, 2048, 1)\n",
        "        self.fc = nn.Linear(2048, voc_size)\n",
        "\n",
        "    def forward(self, x, state=None):\n",
        "        x = self.embedded_train(x)\n",
        "        if state is not None:\n",
        "            x, (h, c) = self.lstm(x, state)\n",
        "        else:\n",
        "            x, (h, c) = self.lstm(x)\n",
        "        x = self.fc(x)\n",
        "        return x, (h, c)\n",
        "\n",
        "\n",
        "def decoding_algo(model, text, leng, data, sample):\n",
        "    model.eval()\n",
        "    state = None\n",
        "\n",
        "    for t in text:\n",
        "        inp = data.data.new_full((1, 1), data.vocab.string_to_id[t]).to(DEVICE)\n",
        "        print(data.vocab.id_to_string[inp.item()], end=\"\")\n",
        "        out, state = model(inp, state)\n",
        "\n",
        "    pred = None\n",
        "    out = torch.nn.functional.softmax(out, dim=2)\n",
        "\n",
        "    if sample:\n",
        "        pred = torch.multinomial(out[0], num_samples=1).item()\n",
        "    else:\n",
        "        pred = torch.argmax(out).item()\n",
        "\n",
        "    print(data.vocab.id_to_string[pred], end=\"\")\n",
        "\n",
        "    for i in range(leng):\n",
        "        pred = data.data.new_full((1, 1), pred).to(DEVICE)\n",
        "        out, state = model(pred, state)\n",
        "        out = torch.nn.functional.softmax(out, dim=2)\n",
        "        if sample:\n",
        "            pred = torch.multinomial(out[0], num_samples=1).item()\n",
        "        else:\n",
        "            pred = torch.argmax(out).item()\n",
        "        print(data.vocab.id_to_string[pred], end=\"\")\n",
        "\n",
        "    print()\n",
        "    return None"
      ],
      "metadata": {
        "id": "ja67eolkv15O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the file exists\n",
        "if os.path.exists(\"books.zip\") and not os.path.exists(\"books\"):\n",
        "    # Extract the contents\n",
        "    with zipfile.ZipFile(\"books.zip\", 'r') as zip_ref:\n",
        "        zip_ref.extractall(\"books\")\n",
        "    print(f\"Contents of 'books.zip' extracted to 'books' folder.\")\n",
        "\n",
        "    folder_path = \"./books/books/\"\n",
        "\n",
        "    # Initialize an empty string to store the combined text\n",
        "    combined_text = \"\"\n",
        "\n",
        "    # Loop through all files in the folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                file_contents = file.read()\n",
        "                combined_text += file_contents + \"\\n\"\n",
        "\n",
        "    # Specify the output file name\n",
        "    output_file = folder_path + \"all_books.txt\"\n",
        "\n",
        "    # Write the combined text to the output file\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as output:\n",
        "        output.write(combined_text)\n",
        "\n",
        "    print(f\"All .txt files in the folder have been combined into '{output_file}'.\")\n",
        "\n",
        "else:\n",
        "    print(f\"'books.zip' does not exist or already unzipped.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8NqVXDVyZ4L",
        "outputId": "e1368de7-7a93-49fb-8a30-24240e6a6459"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'books.zip' does not exist or already unzipped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_path = \"./books/books/all_books.txt\"\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "batch_size = 32\n",
        "bptt_len = 64\n",
        "\n",
        "my_data = LongTextData(text_path, device=DEVICE)\n",
        "\n",
        "batches = ChunkedTextData(my_data, batch_size, bptt_len, pad_id=0)\n",
        "print(batches.__len__())"
      ],
      "metadata": {
        "id": "jc4hEG1-50D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(my_data.vocab.__len__())\n",
        "model.to(DEVICE)\n",
        "\n",
        "lr = 0.001\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=-1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "id": "tj8NRb_U5608"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-IbjbrWvqkP"
      },
      "outputs": [],
      "source": [
        "state = None\n",
        "perplexs = []\n",
        "perp = 100\n",
        "i = 0\n",
        "while perp >= 1.05:\n",
        "    perp = 0\n",
        "    for k in range(batches.__len__()):\n",
        "        out, state = model(batches[k][:-1].to(DEVICE), state)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_function(out.transpose(1, 2), batches[k][1:].to(DEVICE))\n",
        "        state = (state[0].detach(), state[1].detach())\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        perp += math.exp(loss)\n",
        "        perplexs.append(perp)\n",
        "\n",
        "    perp = perp/batches.__len__()\n",
        "    print(\"perplexity epoch \", i, \": \", perp)\n",
        "    text = 'The meaning of life is '\n",
        "    #decoding_algo(model, text, 14, my_data, False)\n",
        "    model.train()\n",
        "\n",
        "    i += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Fox and the Goat '\n",
        "decoding_algo(model, text, 100, my_data, False)"
      ],
      "metadata": {
        "id": "1fYmSa0jv7XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The lion and the Crocodile '\n",
        "decoding_algo(model, text, 150, my_data, False)"
      ],
      "metadata": {
        "id": "XGFnkWn9v8RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'The meaning of the life is '\n",
        "decoding_algo(model, text, 200, my_data, False)"
      ],
      "metadata": {
        "id": "N-Oq9Clnv-AA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}